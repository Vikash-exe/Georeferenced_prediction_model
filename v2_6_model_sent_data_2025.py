# -*- coding: utf-8 -*-
"""V2.6_model_Sent_Data_2025

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HXSS-Tgs3kcHkR-7JEOXStfHdMW9lZK2
"""

# Commented out IPython magic to ensure Python compatibility.
# Cell 1 — install lib if necessary and imports
# (Uncomment installs if packages missing)
# %pip install rasterio tifffile scikit-image

import os
import numpy as np
import torch
from torchvision import transforms
from PIL import Image
import tifffile as tiff
import rasterio
from skimage.exposure import match_histograms

# Cell 0 — mount drive and set paths
from google.colab import drive
drive.mount('/content/drive')

# === Edit these paths to match your environment ===
zip_path = "/content/drive/MyDrive/Test_Dataset_2025/Dataset.zip"   # if you need to unzip
extract_dir = "/content/Sentinel_Test"                             # where zip will be extracted
sentinel_dir = extract_dir   # will autodetect images under this folder
liss_ref_path = "/content/drive/MyDrive/LISS4_ref_Tile/Image_001.tif"  # your LISS-IV reference
output_dir = "/content/drive/MyDrive/Segformer_Predictions"        # where to save predictions in Drive

# Create output dir
import os
os.makedirs(output_dir, exist_ok=True)

# If your data is zipped, unzip to extract_dir (uncomment if needed)
import zipfile
if zip_path and os.path.exists(zip_path):
    os.makedirs(extract_dir, exist_ok=True)
    with zipfile.ZipFile(zip_path, 'r') as z:
        z.extractall(extract_dir)
    print("✅ Extracted zip into:", extract_dir)

print("Sentinel folder (set sentinel_dir):", sentinel_dir)
print("LISS reference (liss_ref_path):", liss_ref_path)
print("Predictions will be saved to:", output_dir)

# %pip install rasterio

# Cell 2 — helper functions

def stretch_to_uint8(img_arr, low_pct=2.0, high_pct=98.0):
    """
    Per-channel percentile stretch -> uint8 (0..255).
    img_arr: H,W,C (float or int).
    """
    if img_arr.ndim == 3 and img_arr.shape[0] in (1,3,4) and img_arr.shape[0] < img_arr.shape[2]:
        # channel-first form -> convert to HWC
        img_arr = np.transpose(img_arr, (1,2,0))
    img = img_arr.astype(np.float32)
    if img.ndim == 2:
        img = np.stack([img]*3, axis=-1)
    if img.shape[2] > 3:
        img = img[..., :3]
    out = np.zeros_like(img, dtype=np.uint8)
    for c in range(img.shape[2]):
        band = img[..., c]
        lo, hi = np.percentile(band, (low_pct, high_pct))
        if hi <= lo:
            scaled = np.clip(band, 0, 255)
        else:
            scaled = (band - lo) / (hi - lo) * 255.0
        out[..., c] = np.clip(scaled, 0, 255).astype(np.uint8)
    return out

def safe_open_and_rescale(path, low_pct=2, high_pct=98):
    """
    Read a TIFF/PNG/JPG and return PIL Image in uint8 RGB after percentile stretch.
    """
    if path.lower().endswith((".tif", ".tiff")):
        arr = tiff.imread(path)
    else:
        arr = np.array(Image.open(path))
    # fix shapes: (C,H,W) => (H,W,C)
    if arr.ndim == 3 and arr.shape[0] in (1,3,4) and arr.shape[0] < arr.shape[2]:
        arr = np.transpose(arr, (1,2,0))
    if arr.ndim == 2:
        arr = np.stack([arr]*3, axis=-1)
    if arr.shape[2] > 3:
        arr = arr[..., :3]
    u8 = stretch_to_uint8(arr, low_pct=low_pct, high_pct=high_pct)
    return Image.fromarray(u8), arr  # return PIL and original numeric array if needed

def load_liss_ref_as_uint8(ref_path, low_pct=2, high_pct=98):
    """
    Load LISS-IV reference tile and convert to uint8 (if not already).
    Returns numpy H,W,3 uint8.
    """
    if ref_path is None or not os.path.exists(ref_path):
        return None
    if ref_path.lower().endswith((".tif", ".tiff")):
        ref_arr = tiff.imread(ref_path)
    else:
        ref_arr = np.array(Image.open(ref_path))
    # normalize shape
    if ref_arr.ndim == 3 and ref_arr.shape[0] in (1,3,4) and ref_arr.shape[0] < ref_arr.shape[2]:
        ref_arr = np.transpose(ref_arr, (1,2,0))
    if ref_arr.ndim == 2:
        ref_arr = np.stack([ref_arr]*3, axis=-1)
    if ref_arr.shape[2] > 3:
        ref_arr = ref_arr[..., :3]
    ref_u8 = stretch_to_uint8(ref_arr, low_pct=low_pct, high_pct=high_pct)
    return ref_u8

def save_class_geotiff_from_source(pred_small, src_raster_path, out_path):
    """
    pred_small: 2D numpy (H_pred, W_pred) — class ids (uint8 or convertible)
    src_raster_path: path to the sentinel source (we copy CRS/transform/size)
    out_path: output path (GeoTIFF) - will contain single uint8 band
    Steps:
      - Load source raster metadata
      - Resize pred_small to source raster width/height using NEAREST
      - Save with source metadata (count=1, dtype=uint8)
    """
    with rasterio.open(src_raster_path) as src:
        meta = src.meta.copy()
        src_w = src.width
        src_h = src.height

    # Resize prediction to original size using nearest neighbor
    pred_img = Image.fromarray(pred_small.astype(np.uint8))
    pred_resized = pred_img.resize((src_w, src_h), resample=Image.NEAREST)
    pred_arr = np.array(pred_resized).astype(np.uint8)

    # Update meta for single-band
    meta.update({
        "count": 1,
        "dtype": "uint8"
    })

    # Write
    with rasterio.open(out_path, "w", **meta) as dst:
        dst.write(pred_arr, 1)

    return out_path

# Cell 3 — load reference LISS image (used for histogram matching)
use_hist_match = True   # set False to disable histogram matching
liss_ref_path = r"/content/drive/MyDrive/LISS4_ref_Tile/Image_001.tif"  # adjust path

if use_hist_match:
    ref_u8 = load_liss_ref_as_uint8(liss_ref_path)
    if ref_u8 is None:
        print("⚠️ LISS ref not found:", liss_ref_path)
    else:
        print("Loaded LISS reference shape (H,W,C):", ref_u8.shape)
else:
    ref_u8 = None
    print("Histogram matching disabled.")

# Cell 4 — Recreate model architecture & load state_dict
import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# === Recreate the same model architecture you used during training ===
# Make sure CustomSegFormer and AttentionUpsample are already defined above
num_classes = 5  # change if different in your training
model = CustomSegFormer(num_classes=num_classes)

# === Load state_dict weights ===
ckpt_path = "/content/drive/MyDrive/2.2_CustomB2_CE+Lovasz_lulc/model_state_dict.pth"  # <-- update to your actual weights path

if os.path.exists(ckpt_path):
    state_dict = torch.load(ckpt_path, map_location=device)
    # In case keys are prefixed with 'module.' from DataParallel, strip them
    new_state_dict = {}
    for k, v in state_dict.items():
        new_k = k.replace("module.", "") if k.startswith("module.") else k
        new_state_dict[new_k] = v
    model.load_state_dict(new_state_dict)
    print("✅ Loaded state_dict into model from:", ckpt_path)
else:
    raise RuntimeError("❌ Weights file not found at: " + ckpt_path)

# Move to device
model.to(device)
model.eval()
print("✅ Model ready for inference.")

# Cell 5 — run inference on all sentinel files in sentinel_dir and save georeferenced predictions
from torchvision import transforms
import torch
import os

sentinel_dir = "/content/Sentinel_Test"   # ensure this points to folder with .tif files
output_dir = "/content/drive/MyDrive/Segformer_Predictions"
os.makedirs(output_dir, exist_ok=True)

# Define transforms (same as training)
transform = transforms.Compose([
    transforms.Resize((512, 512)),   # model training size
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])
])

files = [f for f in sorted(os.listdir(sentinel_dir)) if f.lower().endswith((".tif", ".tiff", ".png", ".jpg", ".jpeg"))]
print("Found files:", files)

for fname in files:
    src_path = os.path.join(sentinel_dir, fname)
    print("\n--- Processing:", fname)
    try:
        # 1) Open + percentile stretch -> uint8 PIL + also keep original numeric arr
        pil_u8, orig_arr = safe_open_and_rescale(src_path, low_pct=2, high_pct=98)

        # 2) Optional histogram matching (use LISS ref): match_histograms accepts HWC arrays
        if use_hist_match and ref_u8 is not None:
            # match_histograms expects numpy arrays
            src_np = np.array(pil_u8)
            matched = match_histograms(src_np, ref_u8, channel_axis=-1)
            pil_u8 = Image.fromarray(np.clip(matched,0,255).astype(np.uint8))

        # 3) Transform -> model input
        x = transform(pil_u8).unsqueeze(0).to(device)  # [1,3,512,512]

        # 4) Forward pass: handle HF-style outputs (.logits) or plain tensor
        with torch.no_grad():
            out = model(x)
            if hasattr(out, "logits"):
                logits = out.logits
            else:
                logits = out
            # logits: [B, num_classes, H_model, W_model]
            preds = torch.argmax(logits, dim=1).squeeze(0).cpu().numpy().astype(np.uint8)  # H_model x W_model

        # 5) Save georeferenced prediction: resize to original raster size and copy georef from original sentinel tile
        out_name = os.path.splitext(fname)[0] + "_pred.tif"
        out_path = os.path.join(output_dir, out_name)
        saved = save_class_geotiff_from_source(preds, src_path, out_path)
        print("✅ Saved prediction:", saved)

    except Exception as e:
        print("❌ Error processing", fname, ":", str(e))

# Cell 6 — quick visual check (display first saved prediction + overlay)
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
import rasterio
import numpy as np

# pick a saved file
saved_files = [f for f in sorted(os.listdir(output_dir)) if f.endswith("_pred.tif")]
print("Saved predictions:", saved_files[:5])
if len(saved_files) == 0:
    print("No saved predictions found in:", output_dir)
else:
    pred_path = os.path.join(output_dir, saved_files[0])
    src_path_example = os.path.join(sentinel_dir, os.path.splitext(saved_files[0])[0].replace("_pred","") + ".tif")
    # read prediction
    with rasterio.open(pred_path) as src:
        pred = src.read(1)
    # read RGB (resampled to same size for display) from source
    with rasterio.open(src_path_example) as s:
        rgb = s.read([1,2,3]).transpose(1,2,0)
        # if large, rescale for display
        if rgb.dtype != np.uint8:
            rgb = stretch_to_uint8(rgb)
    # overlay
    cmap = ListedColormap(['black','blue','green','red','yellow','magenta'])  # adjust colors for classes
    plt.figure(figsize=(12,6))
    plt.subplot(1,2,1); plt.imshow(rgb); plt.title("Original (resampled display)"); plt.axis('off')
    plt.subplot(1,2,2); plt.imshow(pred, cmap=cmap); plt.title("Predicted classes"); plt.axis('off')
    plt.show()

import torch
import torch.nn as nn
from transformers import SegformerModel

class CustomSegFormer(nn.Module):
    def __init__(self, num_classes=5, id2label=None, label2id=None):
        super(CustomSegFormer, self).__init__()

        # === Encoder (pretrained SegFormer B2) ===
        self.encoder = SegformerModel.from_pretrained(
            "nvidia/segformer-b2-finetuned-ade-512-512",
            output_hidden_states=True
        )

        # SegFormer B2 hidden sizes
        embed_dim = [64, 128, 320, 512]
        decoder_dim = 512

        # Projections
        self.linear_c4 = nn.Conv2d(embed_dim[3], decoder_dim, kernel_size=1)
        self.linear_c3 = nn.Conv2d(embed_dim[2], decoder_dim, kernel_size=1)
        self.linear_c2 = nn.Conv2d(embed_dim[1], decoder_dim, kernel_size=1)
        self.linear_c1 = nn.Conv2d(embed_dim[0], decoder_dim, kernel_size=1)

        # Attention-guided upsampling blocks
        self.agu3 = AttentionUpsample(decoder_dim, decoder_dim, decoder_dim)
        self.agu2 = AttentionUpsample(decoder_dim, decoder_dim, decoder_dim)
        self.agu1 = AttentionUpsample(decoder_dim, decoder_dim, decoder_dim)

        # Final classifier
        self.dropout = nn.Dropout(0.1)
        self.classifier = nn.Conv2d(decoder_dim, num_classes, kernel_size=1)

    def forward(self, pixel_values):
        # === Encoder ===
        outputs = self.encoder(pixel_values)
        c1, c2, c3, c4 = outputs.hidden_states  # already (B, C, H, W)

        # Project to same decoder_dim
        c1, c2, c3, c4 = self.linear_c1(c1), self.linear_c2(c2), self.linear_c3(c3), self.linear_c4(c4)

        # === Decoder with AGU ===
        x = self.agu3(c4, c3)  # fuse high-level with C3
        x = self.agu2(x, c2)   # fuse with C2
        x = self.agu1(x, c1)   # fuse with C1

        # Final head
        x = self.dropout(x)
        logits = self.classifier(x)

        # Restore to original image size
        logits = nn.functional.interpolate(logits, size=pixel_values.shape[2:], mode="bilinear", align_corners=False)

        return logits

import torch
import torch.nn as nn

class AttentionUpsample(nn.Module):
    def __init__(self, in_channels, skip_channels, out_channels):
        super().__init__()

        # --- Learnable upsampling instead of bilinear ---
        self.up = nn.ConvTranspose2d(
            in_channels, in_channels, kernel_size=2, stride=2
        )

        # Project input and skip connections to same channels
        self.conv_in = nn.Conv2d(in_channels, out_channels, kernel_size=1)
        self.conv_skip = nn.Conv2d(skip_channels, out_channels, kernel_size=1)

        # Attention map
        self.attn = nn.Sequential(
            nn.Conv2d(out_channels * 2, out_channels, kernel_size=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, 1, kernel_size=1),
            nn.Sigmoid()
        )

        # Final refinement
        self.conv_out = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)

    def forward(self, x, skip):
        # 1. Upsample using ConvTranspose2d
        x = self.up(x)

        # 2. If size mismatch (odd dims), force match
        if x.shape[-2:] != skip.shape[-2:]:
            x = nn.functional.interpolate(x, size=skip.shape[2:], mode="nearest")

        # 3. Project features
        x_proj = self.conv_in(x)
        skip_proj = self.conv_skip(skip)

        # 4. Compute attention mask
        attn_map = self.attn(torch.cat([x_proj, skip_proj], dim=1))

        # 5. Weighted fusion
        out = attn_map * x_proj + (1 - attn_map) * skip_proj
        out = self.conv_out(out)
        return out