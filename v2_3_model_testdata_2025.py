# -*- coding: utf-8 -*-
"""V2.3_model_testData_2025

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Klwg3V9GfINJoNiK18M5zI8Ox73Q-4OK
"""

# If needed, uncomment to install deps
# !pip install -q transformers timm tifffile

import os
import re
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
from PIL import Image
import tifffile as tiff
from tqdm import tqdm
import matplotlib.pyplot as plt
from transformers import SegformerForSemanticSegmentation

# Colab: mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

import zipfile
import os

# Path to your zip file
zip_path = "/content/drive/MyDrive/Test_Dataset_2025/Dataset.zip"
extract_dir = "/content"   # where you want to unzip

# Make sure output dir exists
os.makedirs(extract_dir, exist_ok=True)

# Extract zip
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_dir)

print(f"✅ Dataset extracted to: {extract_dir}")

import os
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import tifffile as tiff
from torchvision import transforms

# ==============================
# Safe image loader for TIFF / PNG / JPG
# ==============================
def safe_open_tiff(path):
    """
    Opens an image from path and returns a PIL Image in RGB.
    Handles grayscale or multi-channel TIFF images.
    """
    img = tiff.imread(path) if path.lower().endswith((".tif", ".tiff")) else np.array(Image.open(path))

    # Ensure 3 channels
    if img.ndim == 2:  # grayscale → RGB
        img = np.stack([img]*3, axis=-1)
    elif img.shape[2] > 3:  # extra channels → take first 3
        img = img[:, :, :3]

    img = np.clip(img, 0, 255).astype(np.uint8)
    return Image.fromarray(img)

# ==============================
# Inference Dataset
# ==============================
class InferenceDataset(Dataset):
    def __init__(self, img_dir, image_size=512, normalize=True):
        self.img_dir = img_dir
        self.image_files = sorted([
            f for f in os.listdir(img_dir)
            if f.lower().endswith((".png", ".jpg", ".jpeg", ".tif", ".tiff"))
        ])
        self.image_size = image_size
        self.normalize = normalize

        # Define transform
        self.transform = transforms.Compose([transforms.Resize((image_size, image_size)),
                                             transforms.ToTensor()])
        if normalize:
            # ImageNet mean/std
            self.transform.transforms.append(
                transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                     std=[0.229, 0.224, 0.225])
            )

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        img_path = os.path.join(self.img_dir, self.image_files[idx])
        img = safe_open_tiff(img_path)
        img_tensor = self.transform(img)
        return img_tensor, self.image_files[idx]

# ==============================
# Example: Loading dataset and dataloader
# ==============================
IMG_DIR = "/content/"
dataset = InferenceDataset(IMG_DIR, image_size=512, normalize=True)
dataloader = DataLoader(dataset, batch_size=1, shuffle=False)

print("✅ Dataset ready. Total images:", len(dataset))

# Commented out IPython magic to ensure Python compatibility.
# %pip install rasterio

import os
import numpy as np
import torch
import rasterio
from rasterio.transform import from_origin
import tifffile
from PIL import Image

# ==============================
# 1. Load trained model
# ==============================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define your model architecture (must match training!)
model = CustomSegFormer(num_classes=5)   # <-- adjust num_classes
model.load_state_dict(torch.load(
    "/content/drive/MyDrive/2.2_CustomB2_CE+Lovasz_lulc/model_state_dict.pth",
    map_location=device
))
model.to(device)
model.eval()

def save_class_geotiff(output_path, pred, ref_path):
    """
    Save prediction (H,W) as a single-band GeoTIFF (values = class IDs),
    preserving georeferencing from ref_path.
    """
    with rasterio.open(ref_path) as src:
        meta = src.meta.copy()

    # Update metadata for single-band integer output
    meta.update({
        "count": 1,
        "dtype": "uint8"
    })

    with rasterio.open(output_path, "w", **meta) as dst:
        dst.write(pred.astype(np.uint8), 1)

# ==============================
# 3. Run inference on folder
# ==============================
def run_inference_folder(model, input_dir, output_dir, device="cuda"):
    os.makedirs(output_dir, exist_ok=True)

    for fname in os.listdir(input_dir):
        if not fname.lower().endswith((".tif", ".tiff", ".png", ".jpg", ".jpeg")):
            continue

        img_path = os.path.join(input_dir, fname)
        print(f"Processing {fname}...")

        # Load image
        img_np = tifffile.imread(img_path) if fname.lower().endswith((".tif", ".tiff")) else np.array(Image.open(img_path))

        # Convert single-channel → RGB
        if img_np.ndim == 2:
            img_np = np.stack([img_np]*3, axis=-1)
        elif img_np.shape[-1] > 3:
            img_np = img_np[..., :3]

        # Resize to 512x512 (⚠️ this breaks original georeferencing resolution!)
        pil_img = Image.fromarray(img_np.astype(np.uint8)).resize((512, 512))
        img_np = np.array(pil_img).astype(np.float32) / 255.0

        # Normalize
        mean = np.array([0.485, 0.456, 0.406]).reshape(1,1,3)
        std  = np.array([0.229, 0.224, 0.225]).reshape(1,1,3)
        img_np = (img_np - mean) / std

        # To tensor
        img_tensor = torch.tensor(img_np).permute(2,0,1).unsqueeze(0).float().to(device)

        # Predict
        with torch.no_grad():
            logits = model(img_tensor)
            pred = torch.argmax(logits, dim=1).squeeze(0).cpu().numpy()

        # Save GeoTIFF (single-band class mask)
        out_path = os.path.join(output_dir, fname.replace(".tif", "_pred.tif"))
        save_class_geotiff(out_path, pred, img_path)


    print(f"✅ All predictions saved to: {output_dir}")

import torch
import torch.nn as nn
from transformers import SegformerModel

class CustomSegFormer(nn.Module):
    def __init__(self, num_classes=5, id2label=None, label2id=None):
        super(CustomSegFormer, self).__init__()

        # === Encoder (pretrained SegFormer B2) ===
        self.encoder = SegformerModel.from_pretrained(
            "nvidia/segformer-b2-finetuned-ade-512-512",
            output_hidden_states=True
        )

        # SegFormer B2 hidden sizes
        embed_dim = [64, 128, 320, 512]
        decoder_dim = 512

        # Projections
        self.linear_c4 = nn.Conv2d(embed_dim[3], decoder_dim, kernel_size=1)
        self.linear_c3 = nn.Conv2d(embed_dim[2], decoder_dim, kernel_size=1)
        self.linear_c2 = nn.Conv2d(embed_dim[1], decoder_dim, kernel_size=1)
        self.linear_c1 = nn.Conv2d(embed_dim[0], decoder_dim, kernel_size=1)

        # Attention-guided upsampling blocks
        self.agu3 = AttentionUpsample(decoder_dim, decoder_dim, decoder_dim)
        self.agu2 = AttentionUpsample(decoder_dim, decoder_dim, decoder_dim)
        self.agu1 = AttentionUpsample(decoder_dim, decoder_dim, decoder_dim)

        # Final classifier
        self.dropout = nn.Dropout(0.1)
        self.classifier = nn.Conv2d(decoder_dim, num_classes, kernel_size=1)

    def forward(self, pixel_values):
        # === Encoder ===
        outputs = self.encoder(pixel_values)
        c1, c2, c3, c4 = outputs.hidden_states  # already (B, C, H, W)

        # Project to same decoder_dim
        c1, c2, c3, c4 = self.linear_c1(c1), self.linear_c2(c2), self.linear_c3(c3), self.linear_c4(c4)

        # === Decoder with AGU ===
        x = self.agu3(c4, c3)  # fuse high-level with C3
        x = self.agu2(x, c2)   # fuse with C2
        x = self.agu1(x, c1)   # fuse with C1

        # Final head
        x = self.dropout(x)
        logits = self.classifier(x)

        # Restore to original image size
        logits = nn.functional.interpolate(logits, size=pixel_values.shape[2:], mode="bilinear", align_corners=False)

        return logits

import torch
import torch.nn as nn

class AttentionUpsample(nn.Module):
    def __init__(self, in_channels, skip_channels, out_channels):
        super().__init__()

        # --- Learnable upsampling instead of bilinear ---
        self.up = nn.ConvTranspose2d(
            in_channels, in_channels, kernel_size=2, stride=2
        )

        # Project input and skip connections to same channels
        self.conv_in = nn.Conv2d(in_channels, out_channels, kernel_size=1)
        self.conv_skip = nn.Conv2d(skip_channels, out_channels, kernel_size=1)

        # Attention map
        self.attn = nn.Sequential(
            nn.Conv2d(out_channels * 2, out_channels, kernel_size=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, 1, kernel_size=1),
            nn.Sigmoid()
        )

        # Final refinement
        self.conv_out = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)

    def forward(self, x, skip):
        # 1. Upsample using ConvTranspose2d
        x = self.up(x)

        # 2. If size mismatch (odd dims), force match
        if x.shape[-2:] != skip.shape[-2:]:
            x = nn.functional.interpolate(x, size=skip.shape[2:], mode="nearest")

        # 3. Project features
        x_proj = self.conv_in(x)
        skip_proj = self.conv_skip(skip)

        # 4. Compute attention mask
        attn_map = self.attn(torch.cat([x_proj, skip_proj], dim=1))

        # 5. Weighted fusion
        out = attn_map * x_proj + (1 - attn_map) * skip_proj
        out = self.conv_out(out)
        return out

input_dir = "/content/"
output_dir = "/content/drive/MyDrive/predictions"

run_inference_folder(model, input_dir, output_dir, device=device)